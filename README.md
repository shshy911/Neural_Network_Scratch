# Neural Network From Scratch (NumPy)

This project implements a fully connected neural network from scratch using only NumPy.

The goal was to understand the mechanics of forward propagation, backpropagation, and gradient descent without relying on deep learning frameworks.

Features

Multi-layer fully connected architecture

ReLU activation for hidden layers

Sigmoid activation for output layer

Binary Cross Entropy loss

Manual backpropagation using the chain rule

Gradient descent updates

Architecture Example

Example configuration used during testing:

Input: 2 features

Hidden Layer 1: 2 neurons

Hidden Layer 2: 2 neurons

Output: 1 neuron

The network successfully learns the XOR problem.

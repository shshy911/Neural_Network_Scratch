import numpy as np
import random
class NeuralNetwork:

    def __init__(self, n_neurons, n_features, lr=0.01):

        self.n_neurons = n_neurons
        self.n_layers = len(n_neurons)
        self.n_features = n_features
        self.lr = lr

        self.weights = []
        self.bias = []

        # First layer
        self.weights.append(np.random.randn(n_features, n_neurons[0]))
        self.bias.append(np.zeros((n_neurons[0], 1)))

        # Remaining layers
        for i in range(1, self.n_layers):
            self.weights.append(np.random.randn(n_neurons[i-1], n_neurons[i]))
            self.bias.append(np.zeros((n_neurons[i], 1)))

    # ---------- ACTIVATIONS ----------

    def relu(self, z):
        return np.maximum(0, z)

    def sigmoid(self, z):
        return 1 / (1 + np.exp(-z))

    # ---------- FORWARD ----------

    def forward(self, X):

        self.z = []
        self.a = []

        A = X

        for l in range(self.n_layers):

            Z = self.weights[l].T @ A + self.bias[l]
            self.z.append(Z)

            if l == self.n_layers - 1:
                A = self.sigmoid(Z)
            else:
                A = self.relu(Z)

            self.a.append(A)

        return A

    # ---------- LOSS (Binary Cross Entropy) ----------

    def loss(self, y):

        y_hat = self.a[-1]
        eps = 1e-8  # avoid log(0)

        return -np.mean(
            y * np.log(y_hat + eps) +
            (1 - y) * np.log(1 - y_hat + eps)
        )

    # ---------- BACKPROP ----------

    def backprop(self, X, y):

        L = self.n_layers

        dW = [None] * L
        db = [None] * L

        error = self.a[L-1] - y   
        for l in range(L-1, -1, -1):

            if l == 0:
                A_prev = X
            else:
                A_prev = self.a[l-1]

            dW[l] = A_prev @ error.T
            db[l] = error

            if l > 0:
                error = self.weights[l] @ error
                error = error * (self.z[l-1] > 0)

        # Update weights AFTER computing all gradients
        for l in range(L):
            self.weights[l] -= self.lr * dW[l]
            self.bias[l] -= self.lr * db[l]

    # ---------- FIT ----------

    def fit(self, X_train, y_train, epochs):

        for epoch in range(epochs):

            epoch_loss = 0.0

            for i in range(X_train.shape[0]):

                xi = X_train[i].reshape(-1, 1)
                yi = y_train[i].reshape(-1, 1)

                self.forward(xi)
                epoch_loss += self.loss(yi)
                self.backprop(xi, yi)

            epoch_loss /= X_train.shape[0]

            print(f"Epoch {epoch+1}/{epochs} - loss: {epoch_loss:.4f}")
